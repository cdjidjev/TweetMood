{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sentiment and Emotion Analysis of COVID-19 Related Tweets Using NLP\n",
    "\n",
    "This notebook analyzes the sentiment and emotion of COVID-19 related tweets collected via Twitter streams. Natural Language Processing (NLP) techniques are used to preprocess the collected tweets and then score them using ML models. \n",
    "\n",
    "The sentiment and emotion scores of each tweet are then averaged over a set of 20 tweets, and the resulting scores are stored in a file. This approach allows for an overall assessment of the sentiment and emotion trends related to COVID-19 on Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/christiedjidjev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christiedjidjev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/christiedjidjev/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christiedjidjev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd\n",
    "import gzip, pickle\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from functions import preprocess, writeFile\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function taken from https://github.com/mhoikka/twitsent/blob/main/twitsent/src/twitsent/parse_sentiment.py\n",
    "def parse(interval_lists):\n",
    "        \"\"\"\n",
    "        Sentiment is parsed from collected tweet text and converted to a score\n",
    "        Parameters\n",
    "        --------\n",
    "        interval_lists : list of lists\n",
    "            Cleaned tweet text grouped by the time interval of data collection\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        all_sentiment : list of lists\n",
    "            Sentiment score data for each tweet in interval_lists, grouped\n",
    "            correspondingly by time interval\n",
    "            \n",
    "        Raises\n",
    "        --------\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #initialize sentiment analysis tool\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        #create lists to store sentiment analysis scores in a format that mimics the interval_lists fed as input to this method\n",
    "        sent_list = []  #temp sublist of all_sentiment\n",
    "        all_sentiment = []  #this list is returned\n",
    "\n",
    "        #list of words that are meaningless to sentiment analysis\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "        #use all meaningful words in the strings within interval_lists to parse sentiment and store the sentiment within all_sentiment\n",
    "        for interval in interval_lists:\n",
    "            for tweet in interval:\n",
    "                sent_list.append(sia.polarity_scores(tweet)['compound'])\n",
    "            all_sentiment.append(sent_list.copy())\n",
    "            sent_list.clear()\n",
    "\n",
    "        return all_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['anger', 'anticipation', 'fear', 'joy', 'love', 'optimism', 'pessimism']\n",
    "avg_labels = {em:[] for em in emotions}\n",
    "\n",
    "# loads a trained model and a vectorizer from a pickle file\n",
    "fileName = '/Users/christiedjidjev/Library/CloudStorage/OneDrive-Personal/Classes/Twitter Sentiment/training.txt'\n",
    "with gzip.GzipFile(fileName, 'rb') as f:\n",
    "    trained_models, vectoriser = pickle.load(f)\n",
    "\n",
    "folder_path = '/Users/christiedjidjev/Library/CloudStorage/OneDrive-Personal/Classes/Twitter Sentiment/twitter data by 20 covid/'\n",
    "\n",
    "avg_labels['positive_sentiment'] = []\n",
    "avg_labels['negative_sentiment'] = []\n",
    "\n",
    "# loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename == '.DS_Store':\n",
    "        continue\n",
    "    f = os.path.join(folder_path, filename)\n",
    "    data = []\n",
    "    \n",
    "    \n",
    "    # Read the text file\n",
    "    with open(f, 'r', encoding= \"ISO-8859-1\") as f:\n",
    "        count = 0\n",
    "        new_tweet = \"\"\n",
    "\n",
    "        for line in f:\n",
    "            count+=1\n",
    "            # Remove new line from 'line'\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            #print(line)\n",
    "            new_tweet += line\n",
    "            if line == \"}\":\n",
    "                try:\n",
    "                    json_dict = json.loads(new_tweet)\n",
    "                    del json_dict['matching_rules']\n",
    "                    data.append(json_dict['data'])\n",
    "                    new_tweet = \"\"\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    df_20 = pd.DataFrame(data)\n",
    "\n",
    "    dataset=df_20[['text']]\n",
    "\n",
    "    #Making statement text in lowercase\n",
    "    dataset = dataset.copy()\n",
    "    dataset['text']=dataset['text'].str.lower()\n",
    "    dataset['text'].tail()\n",
    "\n",
    "    dataset = preprocess(dataset)\n",
    "\n",
    "    #putting words back into a single string\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: ' '.join(x))\n",
    "    dataset['text'].head()\n",
    "    dataset_copy = dataset.copy()\n",
    "    #dataset = dataset_copy\n",
    "\n",
    "    X=dataset.text\n",
    "    X = vectoriser.transform(X)\n",
    "\n",
    "    all_emotions = ['anger','anticipation','disgust','fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "    best_method = {'anger':'SVCmodel', 'anticipation':'SVCmodel', 'fear':'BernoulliNB','joy':'SVCmodel',\\\n",
    "                'love':'SVCmodel','optimism':'log_regression','pessimism':'BernoulliNB'}\n",
    "\n",
    "    emotions = list(best_method.keys())\n",
    "\n",
    "    labels = {}\n",
    "    for em in emotions:\n",
    "        y = trained_models[em][best_method[em]].predict(X)\n",
    "        labels[em] = y\n",
    "\n",
    "\n",
    "    tweet_list = []\n",
    "    for i in range(len(df_20)):\n",
    "        x=dataset['text'][i]\n",
    "        tweet_list.append(x)\n",
    "\n",
    "    tweet_list = [tweet.split() for tweet in tweet_list]\n",
    "\n",
    "    sentiment_list = parse(tweet_list)\n",
    "\n",
    "    #take mean of sentiment scores for each interval, using zero for the mean of any intervals that have no scores due to lack of data\n",
    "    avg_sent = [\n",
    "        \n",
    "        sum(interval) / len(interval) if len(interval) != 0 else 0\n",
    "        for interval in sentiment_list\n",
    "    ]\n",
    "\n",
    "    for em in labels:\n",
    "        lst = [int(x) for x in labels[em]]\n",
    "        avg_labels[em].append(sum(lst)/len(lst))\n",
    "\n",
    "    sentiment_pos = [s if s > 0 else 0 for s in avg_sent]\n",
    "    sentiment_neg = [s if s < 0 else 0 for s in avg_sent]\n",
    "\n",
    "    avg_labels['positive_sentiment'].append(sum(sentiment_pos)/len(sentiment_pos))\n",
    "    avg_labels['negative_sentiment'].append(sum(sentiment_neg)/len(sentiment_neg))\n",
    "\n",
    "writeFile('avg_labels.gzip', avg_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
